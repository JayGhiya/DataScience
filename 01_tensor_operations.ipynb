{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "01-tensor-operations.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayGhiya/DataScience/blob/master/01_tensor_operations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQLUIIOBFjmS",
        "colab_type": "text"
      },
      "source": [
        "**PYTORCH101**  \n",
        "\n",
        "### *Tensors*\n",
        "\n",
        "An short introduction about PyTorch and about the chosen functions. \n",
        "- function 1 - **torch.as_tensor(data, dtype=None, device=None) → Tensor**\n",
        "- function 2 - **torch.stack(tensors, dim=0, out=None) → Tensor**\n",
        "- function 3 - **torch.cat(tensors, dim=0, out=None) → Tensor**\n",
        "- function 4 - **torch.matmul(input, other, out=None) → Tensor**\n",
        "- function 5 - **torch.squeeze(input, dim=None, out=None) → Tensor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTS3aFaQFjmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import torch and other required modules\n",
        "import torch\n",
        "import numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVcW3InnFjmX",
        "colab_type": "text"
      },
      "source": [
        "## Function 1 - torch.as_tensor(data, dtype=None, device=None) → Tensor\n",
        "\n",
        "Converts the data into a torch tensor. If the data is an ndarray of the corresponding dtype and the device is CPU, no copy will be performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIdUncaMFjmX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "2e8f2126-e28e-433f-a720-64cf5dbf33df"
      },
      "source": [
        "# Example 1 - working \n",
        "np_array = numpy.array([[4.,5.,6.],[7.,8.,9.],[10.,11.,12.]])\n",
        "t_1 = torch.as_tensor(data = np_array , device=torch.device('cpu')) \n",
        "t_1.size(),t_1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 3]), tensor([[ 4.,  5.,  6.],\n",
              "         [ 7.,  8.,  9.],\n",
              "         [10., 11., 12.]], dtype=torch.float64))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd6i0DWiFjmc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "d92e852a-0fef-49cb-9161-9924e47e34c6"
      },
      "source": [
        "# Example 2 - working\n",
        "t_1[0][0] = 10\n",
        "np_array"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10.,  5.,  6.],\n",
              "       [ 7.,  8.,  9.],\n",
              "       [10., 11., 12.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ktS5LzbFjme",
        "colab_type": "text"
      },
      "source": [
        "Torch as tensor api supports building tensor from tuple,list,ndarray and scalar but not set. As set does not support duplicate values which may be part of tensor and needed for further computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz9R04r2Fjmf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "fd38815d-f559-44e6-e92c-d1cc5a18ee8a"
      },
      "source": [
        "# Example 3 - breaking (to illustrate when it breaks)\n",
        "torch.as_tensor({1,23,3,4})"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-25996525edd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example 3 - breaking (to illustrate when it breaks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of set"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEZUmgVpFjmh",
        "colab_type": "text"
      },
      "source": [
        "To summarize we have used torch.as_tensor to generate a tensor out of numpy array without copying the data of numpy array by saving space. Also we saw how modifying tensor automatically modifies the numpy array which is a handy thing as we can do direct visualizations on numpy array using matplotlib lib after using operations on tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFwnxXnoFjmi",
        "colab_type": "text"
      },
      "source": [
        "torch.tensor() always copies data. If you have a Tensor data and just want to change its requires_grad flag, use requires_grad_() or detach() to avoid a copy. If you have a numpy array and want to avoid a copy, use torch.as_tensor().\n",
        "\n",
        "```\n",
        "# reference:  https://pytorch.org/docs/stable/torch.html#torch.as_tensor\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9igrVv4qFjmi",
        "colab_type": "text"
      },
      "source": [
        "## Function 2 - torch.stack(tensors, dim=0, out=None) → Tensor\n",
        "It adds new tensors along a new dimension. Now let us try to add a tensor using stack to existing tensor t_1. We will use torch.from_numpy() to generate a new tensor and then use torch.stack() to perform the stack operation along dimension 0.![alt text](https://)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dxlBUHfFjmj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "d3166516-e38c-422c-ec70-8ecfdfe42b3a"
      },
      "source": [
        "# Example 1 - working\n",
        "t_2 = torch.from_numpy(numpy.array([[1.,8.,10.],[12.,14.,16.],[19.,20.,25.]]))\n",
        "t_2\n",
        "t_3 = torch.stack([t_1,t_2])\n",
        "t_3,t_3.size()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[10.,  5.,  6.],\n",
              "          [ 7.,  8.,  9.],\n",
              "          [10., 11., 12.]],\n",
              " \n",
              "         [[ 1.,  8., 10.],\n",
              "          [12., 14., 16.],\n",
              "          [19., 20., 25.]]], dtype=torch.float64), torch.Size([2, 3, 3]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfYR1yEPFjmn",
        "colab_type": "text"
      },
      "source": [
        "The stack function can be used when we are training on a batch of say 2d images which will have 2 dimensions. So what batch operation will do is stack n number of images along 0 dimension for that batch and then iterate over that dimension to get list of images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwHW_A23Fjmn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "402827c1-481d-46f0-b8b5-ba8de13c60dd"
      },
      "source": [
        "# Example 2 - working\n",
        "for image in t_3:\n",
        "  #do training\n",
        "  print(image)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[10.,  5.,  6.],\n",
            "        [ 7.,  8.,  9.],\n",
            "        [10., 11., 12.]], dtype=torch.float64)\n",
            "tensor([[ 1.,  8., 10.],\n",
            "        [12., 14., 16.],\n",
            "        [19., 20., 25.]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v91hT98Fjmq",
        "colab_type": "text"
      },
      "source": [
        "As we are stacking tensors along dimension 0 it is important to understand that tensors that need to be stacked have to be of same operation. if they are not then error will be thrown stating dimensions are not of same size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJUl4GUSFjmq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "5642980d-676c-47e0-ee9e-553649ccc47a"
      },
      "source": [
        "# Example 3 - breaking (to illustrate when it breaks)\n",
        "t_4 = torch.from_numpy(numpy.array([[1.,8.,10.,11.],[12.,14.,16.,17.],[19.,20.,25.,26.]]))\n",
        "t_5 = torch.stack([t_1,t_4])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-b690eace9753>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example 3 - breaking (to illustrate when it breaks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m17.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m19.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m26.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mt_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 3] at entry 0 and [3, 4] at entry 1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVEiIj7FFjmt",
        "colab_type": "text"
      },
      "source": [
        "Here we saw how stack operation of tensor can be used to load multiple examples of same tensor size across a new dimension which is zero by default.\n",
        "\n",
        "\n",
        "```\n",
        "# reference: https://pytorch.org/docs/master/generated/torch.stack.html\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQZaWKvDFjmt",
        "colab_type": "text"
      },
      "source": [
        "It can be used when we are training with multiple examples. For instance can be used to stack 2d images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Iu8JTRrFjmu",
        "colab_type": "text"
      },
      "source": [
        "## Function 3 - torch.cat(tensors, dim=0, out=None) → Tensor\n",
        "\n",
        "Concatenation joins tensors along an existing axis.\n",
        "[Reference](https://deeplizard.com/learn/video/kF2AlpykJGY)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQrUnHe5Fjmv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "fb1c618c-be5d-4e0f-e3bd-4b55f26ace34"
      },
      "source": [
        "# Example 1 - working\n",
        "# here we are creating tensors using random function of torch. What we are specifying as arguments is the size of the tensor.\n",
        "t_6 = torch.randn(2, 2)\n",
        "t_7 = torch.randn(2,2)\n",
        "t_6,t_7,t_6.size()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.4515,  0.4627],\n",
              "         [ 0.5606,  0.0564]]), tensor([[1.4285, 1.0836],\n",
              "         [0.8295, 0.5393]]), torch.Size([2, 2]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDDG1b2EFjmx",
        "colab_type": "text"
      },
      "source": [
        "The concat operation comes very handy where want to merge features from different data source/points in one tensor. Let us look how it is done and what impact does it have on our tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDccJMO7Fjmx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c5d1a81d-b2d7-49cb-dff8-e897c793ee29"
      },
      "source": [
        "# Example 2 - Now let us do concatenation of both the tensors along second dimension\n",
        "t_8 = torch.cat([t_6,t_7],dim=1)\n",
        "t_8,t_8.size()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.4515,  0.4627,  1.4285,  1.0836],\n",
              "         [ 0.5606,  0.0564,  0.8295,  0.5393]]), torch.Size([2, 4]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JYbX_iPFjm0",
        "colab_type": "text"
      },
      "source": [
        "Concatenation can only happen along existing dimensions. if dimension is used which is not there in tensor then error will be thrown regarding the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boM7KmmLFjm0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "031577ff-f1ff-4c20-936f-666df738ffea"
      },
      "source": [
        "# Example 3 - breaking (to illustrate when it breaks)\n",
        "t_8 = torch.cat([t_6,t_7],dim=2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-cf8bdcfadb39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example 3 - breaking (to illustrate when it breaks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt_8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTkPWLDpFjm4",
        "colab_type": "text"
      },
      "source": [
        "Here in this example we saw usage of concatenation for pytorch tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJPv526jFjm5",
        "colab_type": "text"
      },
      "source": [
        "The function has to be used in places where we want to concatenate data from multiple data sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-Zmk6KtFjm5",
        "colab_type": "text"
      },
      "source": [
        "## Function 4 - torch.matmul(input, other, out=None) → Tensor\n",
        "\n",
        "The function is responsible for doing matrix multiplication operations on two tensors. [Reference](https://pytorch.org/docs/stable/torch.html#torch.matmul)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv9oBxnVFjm6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "323c834e-d4c6-45ca-e422-62bf7beb550c"
      },
      "source": [
        "# Example 1 - working\n",
        "t_9 = torch.randn(2,2)\n",
        "t_10 = torch.randn(2,2)\n",
        "t_11 = torch.matmul(t_9,t_10)\n",
        "t_9,t_10,t_11 \n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.5482, 0.6253],\n",
              "         [0.6028, 1.0405]]), tensor([[-0.6773, -0.1903],\n",
              "         [-0.3803,  0.0977]]), tensor([[-0.6091, -0.0432],\n",
              "         [-0.8040, -0.0130]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urBwD2zkFjm8",
        "colab_type": "text"
      },
      "source": [
        "Now let us try to multiply a matrix and vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFZq5N4PFjm8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "f66a6461-4735-4e7b-bb5e-ba068a072971"
      },
      "source": [
        "# Example 2 - working\n",
        "t_12 = torch.randn(4, 4)\n",
        "t_13 = torch.randn(4)\n",
        "t_14 = torch.matmul(t_12, t_13)\n",
        "t_12, t_13, t_14 , t_14.size()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-5.9827e-01, -6.5229e-01,  1.9788e+00,  3.1657e-02],\n",
              "         [ 3.5171e-01, -1.3246e-01,  3.5843e-01, -1.0040e+00],\n",
              "         [ 1.0360e+00,  9.9712e-04,  9.0087e-01,  3.4729e-01],\n",
              "         [-2.0146e-01,  1.5975e+00,  1.0360e+00, -1.2339e-01]]),\n",
              " tensor([ 1.5434,  2.3169,  1.1878, -0.3348]),\n",
              " tensor([-0.0948,  0.9978,  2.5550,  4.6622]),\n",
              " torch.Size([4]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJEya4fgFjm_",
        "colab_type": "text"
      },
      "source": [
        "Matrix multiplication is only possible when number of columns from the first matrix are equal to number of rows from the second column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iohisihFjm_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "8420ed01-1d69-48f4-b25d-baab58d5e0d1"
      },
      "source": [
        "# Example 3 - breaking (to illustrate when it breaks)\n",
        "t_15 = torch.randn(4,5)\n",
        "t_16 = torch.randn(4,6)\n",
        "t_17 = torch.matmul(t_15,t_16)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-02efff1ce464>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt_15\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt_16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mt_17\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [4 x 5], m2: [4 x 6] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:41"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA0E2ZTtFjnC",
        "colab_type": "text"
      },
      "source": [
        "The matrix multiplication operation is the most used one in terms of deep learning. For instance it also is used in simple problems like linear regression where the target variable's equation is y=mx+b."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3zkTG4bFjnC",
        "colab_type": "text"
      },
      "source": [
        "The function should be used while doing matrix multiplication."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYlIzU1oFjnD",
        "colab_type": "text"
      },
      "source": [
        "## Function 5 - torch.squeeze(input, dim=None, out=None) → Tensor\n",
        "\n",
        "Squeeze function will get rid of dimensions which have a value 1. It may save lot of time while dealing with tensors without the dimension with value 1. [Reference](https://www.codementor.io/@packt/how-to-perform-basic-operations-in-pytorch-code-10al39a4c4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEgK28QTFjnD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "b22a4e2d-b7c6-47bf-d1e2-f3df248f1766"
      },
      "source": [
        "# Example 1 - working\n",
        "#we will use tensor ones to create a tensor\n",
        "t_18 = torch.ones(5,1,2)\n",
        "t_18"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 1.]],\n",
              "\n",
              "        [[1., 1.]],\n",
              "\n",
              "        [[1., 1.]],\n",
              "\n",
              "        [[1., 1.]],\n",
              "\n",
              "        [[1., 1.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDeegUb0FjnG",
        "colab_type": "text"
      },
      "source": [
        "now let us get rid of extra dimension with a value of 1 to simplify further operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43wQ44-CFjnG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "12dc87ad-3a27-441f-af7e-90944cedd6ff"
      },
      "source": [
        "# Example 2 - \n",
        "t_19  = torch.squeeze(t_18)\n",
        "t_19"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biOCukiAFjnK",
        "colab_type": "text"
      },
      "source": [
        "Squeeze only works with matrices having dimension with value 1. otherwise it will not do any operation. Squeeze also works on existing dimensions. So specifying a dimension which is not preexisting on tensor will throw out a error as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yunq-dFkFjnL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "2ddb8543-e677-494d-8ea0-0f54ad469c0a"
      },
      "source": [
        "# Example 3 - breaking (to illustrate when it breaks)\n",
        "t_20 = torch.squeeze(torch.ones(2,2),dim=2)\n",
        "t_20"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-0a7ca9a3e14b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example 3 - breaking (to illustrate when it breaks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt_20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mt_20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAFuu-2JFjnN",
        "colab_type": "text"
      },
      "source": [
        "Here we saw the basic usage of squeeze to save time in unnecessary operations with dimensions having value 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LTnBHw-FjnN",
        "colab_type": "text"
      },
      "source": [
        "The function comes very handy when we have a training approach to process samples one by one. let us say this is the size (200 * 1 * 100) where 200 is the number of text samples. In a nlp problem, we are looking at one word at a time so there is 1 in the tensor. In order to simplify the further operations on tensor we can get rid of extra dimension and make our life simple."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDXZ8NPWFjnO",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Summarize what was covered in this notebook, and where to go next"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq_vauooFjnO",
        "colab_type": "text"
      },
      "source": [
        "## Reference Links\n",
        "\n",
        "* Official documentation for `torch.Tensor`: https://pytorch.org/docs/stable/tensors.html\n",
        "* https://www.codementor.io/@packt/how-to-perform-basic-operations-in-pytorch-code-10al39a4c4\n",
        "*   https://deeplizard.com/learn/video/fCVuiW9AFzY\n",
        "*   https://towardsdatascience.com/building-efficient-custom-datasets-in-pytorch-2563b946fd9f\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceG7MHfgFjnP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2c5a745b-3f99-4bfd-8016-06e8d55ee53f"
      },
      "source": [
        "!pip install jovian --upgrade --quiet"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |████                            | 10kB 21.3MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 2.3MB/s \n",
            "\u001b[?25h  Building wheel for uuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed5AxXUVFjnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import jovian"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbzC9UZzFjnU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a002206-a469-42c1-8cb6-d35dfffe2a24"
      },
      "source": [
        "jovian.commit()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31m[jovian] Error: Failed to detect Jupyter notebook or Python script. Skipping..\u001b[0m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XrJa2sQFjnX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}